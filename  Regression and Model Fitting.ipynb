{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likehood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation of the Regression Problem\n",
    "\n",
    "Given a multidimensional data set drawn from some pdf and the full error covariance matrix for each data point, we can attempt to infer the underlying pdf using either parametric or nonparametric models. In its most general incarnation, this is avery hard problem to solve. Even with a restrictive assumption that the errors are Gaussian, incorporating the error covariance matrix within the posterior distribution is not trivial. Furthermore, accounting for any selection function applied to the data can increase the computational complexity significantly, and non-Gaussian error behavior, if not accounted for, can produce biased results. \n",
    "\n",
    "Regression addresses a slightly simpler problem: instead of determining the multidimensional pdf, we wish to infer the expectation value of y given x (i.e., the conditional expectation value). If we have a model for the conditional distribution (described by parameters θ ) we can write this function 2 as y = f (x|θ). \n",
    "\n",
    "We refer to y\n",
    "as a scalar dependent variable and x as an independent vector. Here x does not need\n",
    "to be a random variable (e.g., x could correspond to deterministic sampling times for\n",
    "a time series). For a given model class (i.e., the function f can be an analytic function\n",
    "such as a polynomial, or a nonparametric estimator), we have k model parameters\n",
    "θ p , p = 1, . . . , k.\n",
    "Figure 8.1 illustrates how the constraints on the model parameters, θ , respond\n",
    "to the observations x i and y i . In this example, we assume a simple straight-line\n",
    "model with y i = θ 0 + θ 1 x i . Each point provides a joint constraint on θ 0 and θ 1 . If\n",
    "there were no uncertainties on the variables then this constraint would be a straight\n",
    "line in the (θ 0 , θ 1 ) plane (θ 0 = y i − θ 1 x i ). As the number of points is increased\n",
    "the best estimate of the model parameters would then be the intersection of all\n",
    "lines. Uncertainties within the data will transform the constraint from a line to a\n",
    "distribution (represented by the region shown as a gray band in figure 8.1). The best\n",
    "estimate of the model parameters is now given by the posterior distribution. This is\n",
    "simply the multiplication of the probability distributions (constraints) for all points\n",
    "and is shown by the error ellipses in the lower panel of figure 8.1. Measurements with\n",
    "upper limits (e.g., point x 4 ) manifest as half planes within the parameter space. Priors\n",
    "are also accommodated naturally within this picture as additional multiplicative\n",
    "constraints applied to the likelihood distribution (see § 8.2).\n",
    "Computationally, the cost of this general approach to regression can be pro-\n",
    "hibitive (particularly for large data sets). In order to make the analysis tractable, we\n",
    "will, therefore, define several types of regression using three “classification axes”:\n",
    "Linearity. When a parametric model is linear in all model parameters, that is,\n",
    "\u0001\n",
    "f (x|θ ) = kp=1 θ p g p (x), where functions g p (x) do not depend on any free\n",
    "model parameters (but can be nonlinear functions of x), regression becomes a\n",
    "significantly simpler problem, called linear regression. Examples of this include\n",
    "polynomial regression, and radial basis function regression. Regression of\n",
    "models that include nonlinear dependence on θ p , such as f (x|θ ) = θ 1 +\n",
    "θ 2 sin(θ 3 x), is called nonlinear regression.\n",
    "• Problem complexity. A large number of independent variables increases the\n",
    "complexity of the error covariance matrix, and can become a limiting factor\n",
    "in nonlinear regression. The most common regression case found in practice\n",
    "is the M = 1 case with only a single independent variable (i.e., fitting a straight\n",
    "line to data). For linear models and negligible errors on the independent\n",
    "variables, the problem of dimensionality is not (too) important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
